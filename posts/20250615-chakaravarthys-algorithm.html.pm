#lang pollen

â—Š(define-meta title "Chakaravarthy's algorithm")
â—Š(define-meta date "Monday July 28, 2025")

â—Špost-title{â—Š(select-from-metas 'title metas)}
â—Špost-date{â—Š(select-from-metas 'date metas)}

Thankfully, spring quarter is over and summer is here!â—Šfn[0.5] This past quarter was pretty roughâ€”I think I took one class too many, so even though all my classes were really cool, there were moments of real misery.â—Šfn[1] Many moments, actually. The bright side, of course, is that now I have fewer classes to take until I finish up the MS degree. ãƒ„

â—Šem{Edit: I started writing this on June 15, which was right after school ended. But finishing the whole thing ended up taking a bit longer.}

â—Šfndef[0.5]{Having summer break is one the great perks of being in school.}
â—Šfndef[1]{In hindsight though, I still think it was the right decision. The classes were all very interesting, and (I think are) usually only offered in the spring. I'd rather be miserable this spring than next spring! Plus all the classes next quarter look boring, so I'm glad I knocked out more of my credits this school year.}
â—Špost-section{Points-to analysis}
One of my classes this quarter was â—Šlink["https://cs.ucdavis.edu/schedules-classes/ecs-240-programming-languages"]{ECS240: Programming Languages}, which was primarily focused on static analysisâ€”specifically, basic dataflow analysis and abstract interpretation.â—Šfn[2] Something that I found quite cool was this algorithm for â—Šem{polynomial-time} and â—Šem{precise} flow-insensitive points-to analysis of â—Šem{typed} programs, which was introduced in 2003 POPL paper called â—Šlink["https://dl.acm.org/doi/abs/10.1145/640128.604142"]{"New Results on the Computability and Complexity of Points-to Analysis"}.

â—Šfndef[2]{It's pretty funny how many different things a class titled "Programming Languages" can cover. I would usually expect (and probably prefer) a core/required class focused on programming languages to cover "foundational" programming language theory, i.e. things like lambda calculus, type systems and basic results there, semantics, Hoare logic, and so on, but then there's this whole body of material on what could maybe be called â—Šem{implementation theory}â€”things related to compiler optimization, static analysis (particularly of programs in existing languages), garbage collection, etc. that are also really interesting and valid directions a course on programming languages can take.}

There are lots of other goodies about some of the theoretical boundaries of various classes of points-to analysis (the title is very apt!), but this algorithmâ€”sometimes called â—Šstrong{Chakaravarthy's algorithm}â€”is particularly interesting. There really just aren't that many algorithms for non-trivial program analysis that are â—Šem{polynomial-time}, i.e. â—Šem{very fast}! It's also very nice that â—Šem{types} (if only a very limited notion of types) are what makes it all possible.

â—Šaside{The algorithm feels a bit underappreciated, particularly since â—Šem{precise} flow-insensitive points-to analysis in the â—Šem{untyped} setting is known to be $\mathcal{NP}$-hard, which is much, much worse from a pure complexity standpoint. It's probably because Andersen-style pointer analysis, a sound and practically good-enough approximation of the precise analysis, has become so fast and practical (for example, see â—Šlink["https://hardekbc.github.io/files/hardekopf07ant.pdf"]{"The Ant and the Grasshopper"} by Hardekopf and Lin from PLDI 2007), and is maybe also easier to teach and compare alongside Steensguard's algorithm, another commonâ—Šfn[3] algorithm for (approximate, untyped, flow-insensitive) pointer analysis.}

â—Šfndef[3]{I actually don't know how "common" any of these algorithms are, although Steensguard is â—Šlink["https://releases.llvm.org/8.0.0/docs/AliasAnalysis.html#the-steens-aa-pass"]{definitely available in LLVM} and I believe lots of current research on this sort of pointer analysis is Andersen-style.}

That said, I honestly don't think the paper does a great job explaining the algorithm, particularly due to (1) some weird organizational quirks that are perhaps due to an effort to present formal complexity proofs alongside the algorithm structure and (2) some unfortunate typos.â—Šfn[3] So here's an attempt at going over the algorithm in a way that feels clearer to me. The full details remain in the paper itselfâ€”we only aim to give a better sense of the â—Šem{structure of} and â—Šem{general intuition behind} the algorithm.

â—Šfndef[3]{Also some math paper-y proofs that rely on some claim of obviousness, i.e. "It is clear that $G'$ is also layered" or "It is easy to see that the algorithm runs in polynomial time." Some of these may indeed be relatively obvious, but I still wish fewer papers would do this and at least sketch the ideas of the proof or give some brief intuition.}

Before we get into the algorithm itself though, let's pin downâ€”roughly, at leastâ€”what we mean by â—Šem{precise flow-insensitive points-to analysis}:
â—Šol{
  â—Šli{â—Šstrong{points-to analysis:} Points-to analysis is a classic problem in static code analysis; it asks the question, "Given a program and two variables $p$ and $q$, can $p$ point to $q$ in some execution of the program?" Knowing what variables can point to what other variables is important for various â—Šem{compiler optimizations}, as well as for other static analyses like â—Šlink["https://en.wikipedia.org/wiki/Constant_folding"]{constant-propagation}.

Importantly, if we know that $p$ â—Šem{cannot} point to $q$, then we gain a guarantee of "separation"â€”$p$ and $q$ must refer to disjoint locations in memoryâ€”that allows us to show the â—Šem{safety} of certain compiler optimizations or improve the â—Šem{precision} of other static analyses. As a simple example of a compiler optimization that having sound points-to knowledge unlocks, consider this scenario: if we know that no other variables in our program point to $q$ and $q$ is assigned some constant integer like $3$, we can pre-compute an arithmetic expression like $q + 2$ at compile time (the expression will always evaluate to $5$), thus speeding up execution at runtime.}
  â—Šli{â—Šstrong{flow-insensitive:} Flow-insensitive points-to analysis is a slightly relaxed version of the points-to problem, where we consider programs as â—Šem{sets of statements} (i.e. the statements have no notion of ordering corresponding to a control-flow graph of the program). We are allowed to execute the statements in our set â—Šem{in any order} and statements can be executed â—Šem{multiple times} (regardless of how many times it actually appears in our program). Note that the dual of this analysis, â—Šem{flow-sensitive} points-to analysis, does take into account the ordering of statements given by a control-flow graph of the program.

Why do we take this relaxed and less precise view of the problem? The simple answer is that flow-sensitive points-to analysis is â—Šem{very computationally expensive} to perform on programs of realistic size. So we have to compromise! And flow-insensitive points-to analysis turns out to still be useful enough in practice.}
  â—Šli{â—Šstrong{precision:} When we talk about an algorithm that gives us â—Šem{precise} flow-insensitive points-to analysis, we mean an algorithm that gives us â—Šem{exactly} the set of points-to relations for which there exists some sequence of program statements (under our relaxed, flow-insensitive formulation of the points-to problem) that â—Šem{realizes} each relation.

Unfortunately, in the general case, precise flow-insensitive analysis is still relatively expensiveâ€”in fact, it's known to be $\mathcal{NP}$-hard! As a result, commonly used points-to algorithmsâ€”like those proposed by Andersen or Steensguardâ€”are actually just sound/safe â—Šem{approximations}, i.e. they include all the points-to relations in the precise set, but may also contain some extra relations.}
}

â—Špost-section{Chakaravarthy's algorithm}
We begin by giving some intuition on why the algorithm enjoys such good algorithmic complexity while still being precise, as well as a high-level view of its structure. We then fill in some of the details of this general outline, using the various definitions and pieces of intuition we've provided. We conclude with a brief sketch of the complexity argument.

â—Špost-subsection{General intuition}
The key to the algorithm is the observation that, by restricting ourselves to well-typed pointers, our pointers form a â—Šem{layered graph} with certain good properties around â—Šem{forbidden pairs}.

â—Šstrong{First, what do we mean by â—Šem{well-typed} pointers?}
We mean that we are restricting ourselves to a model where all:
â—Šol{
  â—Šli{variables have well-defined types, i.e. â—Šcode{int}, â—Šcode{int*}, â—Šcode{int**} in C-style pointer syntax, and}
  â—Šli{pointers are well-typed in the sense that â—Šem{a variable can only point to other variables of a compatible type}, i.e a variable of type â—Šcode{int**} can â—Šem{only} point to a variable of type â—Šcode{int*}.}
} 
In this model (a model that seems quite reasonable, frankly), we can further simplify our notation by simply tracking the number of dereferences â—Šcode{*} that a variable's type has. For example, we say that a variable with type â—Šcode{int**} has the corresponding type $2$, the variable with type â—Šcode{int****} has the corresponding type $4$.

Intuitively, this simplification is fine since we can split any program into independent sets of statements that â—Šem{only} include one base type (i.e. â—Šcode{int} or â—Šcode{string}) and analyze each of those sets separately, since all pointers must be well-typed (and so a â—Šcode{int} pointer can never point to a â—Šcode{string}, vice-versa, et-cetera).

Additionally (â—Šstrong{and importantly}), this also restricts the â—Šem{kinds of program statements} we need to consider. In fact, we only consider two kinds of statements:
â—Šitems{
  â—Šitem{Statements of the form â—Šcode{*(d)p = &z}}
  â—Šitem{and statements of the form â—Šcode{*(d1)p1 = *(d2)p2}.}
}

â—Šstrong{Second, why does this mean that our realizability graph is â—Šem{layered}?}
Define our layers as follows: a variable of type $1$ is in layer $1$, a variable of type $2$ is in layer $2$, and so on. In a realizability graph, realizable points-to relations are edges between variables (where an edge from variable $a$ to variable $b$ means that $a$ can â—Šem{point-to} $b$).

Given our requirement that all pointers are well-typed, note that our graph then has the following properties:
â—Šitems{
  â—Šitem{Any edge in our realizability graph â—Šem{must be} from layer $i$ to layer $i-1$. This is the â—Šem{layering} property.}
  â—Šitem{Our realizability graph must also be â—Šem{acyclic}, as a consequence of the layering property and the fact that all our variables must have well-defined types (so a variable cannot simultaneously have more than one valid type).}
}

â—Šstrong{Finally, what are â—Šem{forbidden pairs}, why are they a problem, and how do our types help?}
Forbidden pairs are also called â—Šem{dependencies} in the paper, which is slightly confusing. Very briefly, â—Šem{forbidden pairs} are basically two points-to relations (or edges, in the visual terminology of our realizability graph) that â—Šem{cannot be simultaneously realized}.

Such forbidden pairs are the â—Šstrong{crux of the difficulty} in getting â—Šem{precise} analysis in the general case, i.e. in the untyped setting that analyses like Andersen's algorithm operate in. In an algorithm like Andersen's, we have no easy way of tracking such forbidden pairings/dependencies. So a â—Šstrong{points-to relation} computed by Andersen's algorithm â—Šstrong{that relies on two relations in a forbidden pair} â—Šem{is allowed}, even though that relation is actually â—Šem{unrealizable} (i.e. because the two points-to relations in the forbidden pair are not simulateously realizable, there actually is no sequence of statements that can actually realize the computed points-to relation that relies on the forbidden pair).

â—Šaside{As a brief example, say we have some statement â—Šcode{d = *a} and two points-to relations already in our points-to set: $\langle a, b \rangle$ and $\langle b, c \rangle$. In Andersen's algorithm, this set of conditions allow us to conclude that $\langle d, c \rangle$.â—Šfn[3.1] However, say that $\langle a, b \rangle$ and $\langle b, c \rangle$ are in a forbidden pair and are thus not realizable simultaneously. This means that $\langle d, c \rangle$ is actually â—Šem{not realizable}, but since there's no easy way to compute and track these sorts of forbidden pairs in the untyped setting, Andersen's algorithm will just let this slide (and accept the resulting imprecision).}
â—Šfndef[3.1]{For reference, take any inference rule style presentation of Andersen's algorithm, which should be accessible with a quick web search or LLM query ðŸ™ƒ}

This is the source of the imprecision of such algorithms: we will compute certain points-to relations for which there is actually no sequence of statements demonstrating realizability. These relations end up being the "extra" relations mentioned earlier.

Getting the truly precise set of points-to relations involves â—Šem{an efficient method of computing and tracking forbidden pairs}. Thankfully, our well-typed pointers and the resulting layered realizability graph gives us such a method. More specifically, â—Šstrong{our layered realizability graph has the following nice property} regarding forbidden pairs:
â—Šitems{
  â—Šitem{Edges (in other words, points-to relations) in different layers can â—Šem{never be dependent} (in other words, can never be in a forbidden pair together). As a note on the definition of the â—Šem{layer} of an edge: an â—Šem{edge} in our graph $\langle a,b \rangle$ where $a$ is in layer $l$ and $b$ is in layer $l-1$ is said to be in layer $l$.}
}
This simplifies things greatly, since â—Šstrong{forbidden pairs of points-to relations can â—Šem{only} occur when they are in the â—Šem{same layer} of our realizability graph}, i.e. edges can only be dependent when they are in the same layer. This gives us an efficient way of computing, tracking, and avoiding such forbidden pairs in the typed setting , helping us keep the algorithm in polynomial time.

â—Špost-subsection{Algorithm structure}
The algorithm proceeds â—Šem{top-down}, in â—Šem{one pass}, through our layered graph structure, starting at the top layer $L$ (the layer containing the largest types) and moving down. We construct the edges in the graph and add to the graph's metadata (i.e. our set of â—Šem{forbidden pairs}) dynamically as we move down the layers. We will need the information from prior layers to perform the algorithm at the current layer.

â—Šaside{Specifically, we assume by induction that all edges and forbidden pairs from layer $L$ (the top layer) to $l+1$ (the preceding layer) have already been correctly computed, since the algorithm at each layer relies on this being the case.}

At each layer $l$, the algorithm proceeds in 4 stages. The first two stages prepare the information necessary for the final two stages, which are the ones that actually compute the new edges and the set of forbidden pairs of edges in the current layer. Here are what each of the 4 stages â—Šem{should do}, in brief:

â—Šol{
  â—Šli{â—Šunderline{Direct assignments:} Take statements of the form â—Šcode{*(d)p = &z}, where â—Šcode{*(d)p} means â—Šcode{d} number of dereferences â—Šcode{*} before â—Šcode{p}. Find the variables â—Šcode{q} in layer $l$ that can be made to point to â—Šcode{z} by this statement. â—Šcode{q} can be made to point to â—Šcode{z} â—Šstrong{iff} there exists a path from â—Šcode{p} to â—Šcode{q} in our realizability graph so far. Note that, for this to work, â—Šcode{z} must be in layer $l-1$ and â—Šcode{p} must be in layer $l+$â—Šcode{d}.} 
  â—Šli{â—Šunderline{Copying statements:} Take statements of the form â—Šcode{*(d1)p1 = *(d2)p2}. For any two pointers â—Šcode{q1} and â—Šcode{q2} in layer $l$, the idea is that such a statement can â—Šem{copy} values of â—Šcode{q2} to â—Šcode{q1} if â—Šem{there is a path from â—Šcode{p1} to â—Šcode{q1} and a path from â—Šcode{p2} to â—Šcode{q2}}, such that the paths are (a) â—Šem{vertex disjoint} and (b) â—Šem{contain no forbidden pairs between them}. Find all these pairs â—Šcode{(q1,q2)} in layer $l$ such that the value of â—Šcode{q2} can be copied to â—Šcode{q1}.}
  â—Šli{â—Šunderline{Edge computation:} Compute the edges in the current layer using the pairs from the â—Šunderline{direct assignments} and â—Šunderline{copying statements} phases.}
  â—Šli{â—Šunderline{Forbidden pair computation:} Compute the forbidden pairs of edges in current layer, again using the pairs we've already computed in the â—Šunderline{direct assignments} and â—Šunderline{copying statements} phases.}
}
Importantly, each of these stages can be modelled as a certain sub-problem with a polynomial time algorithmic solution. Here are each of the stages mapped onto their particular sub-problemâ€”these sub-problems get us from what the stages â—Šem{should do} to an idea of â—Šem{how to do them}:
â—Šol{
  â—Šli{â—Šunderline{Direct assignments:} the â—Šstrong{graph reachability} problem on the realizability graph at that point.}
  â—Šli{â—Šunderline{Copying statements:} the â—Šstrong{disjoint paths with forbidden pairs} problem on the realizability graph at that point.}
  â—Šli{â—Šunderline{Edge computation:} the â—Šstrong{1-CCP (Concurrent Copy Propagation)} problem, using information produced by the â—Šunderline{direct assignments} and â—Šunderline{copying statements} phases.}
  â—Šli{â—Šunderline{Forbidden pair computation:} the â—Šstrong{2-CCP} problem, using information produced by the  â—Šunderline{direct assignments} and â—Šunderline{copying statements} phases.}
}

â—Šstrong{Graph reachability} is relatively well-known (and can be implemented fairly simply, if naively, with some kind of bread-first or depth-first search), so we elide deeper discussion of the problem. It's also known to be solvable in at least â—Šem{linear time}.â—Šfn[4] However, we will outline the â—Šstrong{disjoint paths with forbidden pairs} and â—Šstrong{CCP} problems below, along with informal arguments that they can be solved in polynomial time.
â—Šfndef[4]{Since BFS is technically linear time, for example, or more specifically $\mathcal{O}(V+E)$ for some graph $G = (V, E)$. Or an appeal to Wikipedia citations â—Šlink["https://en.wikipedia.org/wiki/Reachability#cite_note-4"]{here}.}

â—Špost-subsection{Disjoint paths with forbidden pairs}
While the usual disjoint paths problem (i.e. where the graph is not necessarily layered and there are no forbidden pairs) is $\mathcal{NP}$-complete, there exists an algorithm for our particular caseâ€”where we have a layered graph and each forbidden pair of edges is in the same layerâ€”that is polynomial time. In particular, we are able to reduce the problem to â—Šstrong{reachability in directed graphs}, using a â—Šem{polynomial time} reduction.

Here are the broad strokes of the algorithmic solution:
â—Šol{
  â—Šli{We take as input our layered graph $G=(V, E)$, with some layering function $l$, a pair of source vertices $(s1,s2)$, a pair of target vertices $(t1,t2)$, and a set of forbidden pairs $F$.}
  â—Šli{We can assume (without loss of generality) that our source vertices are in the same layer and that the target vertices are also in the same layer. (The fact that this is safe is discussed in more depth in the paper.)}
  â—Šli{We now construct a new graph $G'=(V',E')$ using $G$. Formally, we define:
    â—Šol{
      â—Šli{$V'=\{\langle u,v \rangle\ |\ u,v\in V, u \neq v, l(u)=l(v)\}$}
      â—Šli{$E'=\{(\langle u_1,v_1 \rangle, \langle u_2,v_2 \rangle)\ |\ (\langle u_1,u_2 \rangle, \langle v_1,v_2 \rangle) \in (E \times E) - F\}$}
    }
    This is a bit of a mouthful, so take some time to digest these definitions. In plainer words, our new $V'$ is a subset of $V \times V$ and consists of all pairs of distinct vertices in the same layer of $G$. The new "vertices" in $V'$ are connected by an "edge" in $E'$ if the â—Šem{corresponding components of these vertices are connected by a pair of non-forbidden edges in $G$}.
  }
  â—Šli{Now, for the most important bit: a given instance has a solution â—Šstrong{iff there is a path from $\langle s_1,s_2 \rangle$ to $\langle t_1,t_2 \rangle$ in $G'$}. As we foreshadowed earlier, this is just a graph reachability problem in a directed graph.}
}

The reduction is polynomial time, the size of $G'$ is polynomial in the size of $G$, and the resulting reachability problem can then also be solved in polynomial time (i.e. with depth-first search). We leave more formal arguments for the correctness of this reduction to the full paper.

â—Špost-subsection{Concurrent Copy Propagation (1- and 2-CCP)}
The general $k$-CCP problem can be described as follows. We are given a set of variables $V$, a set of constants $C$, and a set of statements $S$. We consider two types of statements: (1) â—Šcode{X := a} for some â—Šcode{X} in $V$ and some â—Šcode{a} in $C$; (2) â—Šcode{X := Y} for some â—Šcode{X} in $V$ and some â—Šcode{Y} in $V$. Executing a statement of type (1) â—Šem{assigns the constant â—Šcode{a} to variable â—Šcode{X}}; executing a statement of type (2) â—Šem{copies the current value of variable â—Šcode{Y} to â—Šcode{X}}.

Now, consider some set of goals $G$ that consists of pairs of variables and constants, i.e. â—Šcode{<X1,a1>,<X2,a2>,...<Xk,ak>}. Note that the â—Šcode{k} here is the same as the $k$ of our $k$-CCP problem (and is just some positive integer).

$G$ can be â—Šem{realized} if there is some finite sequence of statements in $S$ such that, at the end of the sequence, all of our goals in $G$ are realized, i.e. the value of â—Šcode{X1} is â—Šcode{a1}, the value of â—Šcode{X2} is â—Šcode{a2}, ...the value of â—Šcode{Xk} is â—Šcode{ak}. (Note the parallels here to the points-to analysis that we are interested in solving!)

The solution $\lambda$ to $k$-CCP is the â—Šem{set of all such realizable sets of $k$ goals}.

What's interesting about the $k$-CCP problem is that, when $k$ can be arbitrary (and part of the input to the problem), the decision version of the problem (i.e. can our set of goals $G$ be realized?) is PSPACE-complete, but when $k$ is constant, the problem is polynomial time. We are actually â—Šstrong{only interested in the cases where $k=1$ and $k=2$, so our problems have polynomial time solutions}.

For the 1-CCP problem, we can reduce it to a problem of reachability in directed graphs. More specifically, we can construct a graph where statements like â—Šcode{X := Y} are represented by an edge from some variable node â—Šcode{Y} to some variable node â—Šcode{X} and statements like â—Šcode{X := a} are represented by an edge from some constant node â—Šcode{a} to some variable node â—Šcode{X}. To check which variables can be assigned what constants, we simply need to check which variable nodes are reachable from what constant nodes in our graph.

For the 2-CCP problem, here's a sketch of the algorithmic solution, which takes the form of iterative transitive closure based on a set of rules:
â—Šol{
  â—Šli{We start with $\lambda = \{$ â—Šcode{{<X,x>,<Y,y>}}$\ |\ $ â—Šcode{X != Y} and â—Šcode{X := x}, â—Šcode{Y := y} are statements in $S\}$}
  â—Šli{Then we simply iterate over our set of statements $S$, adding more elements to $\lambda$ until we reach a fixpoint. Our rules for adding elements are as follows. For each element in $\lambda$, â—Šcode{{<X,x>,<Y,y>}}:
    â—Šul{
      â—Šli{for a statement of the form â—Šcode{Z := z}, where â—Šcode{Z != X}, add â—Šcode{{<X,x>,<Z,z>}} to $\lambda$}
      â—Šli{for a statement of the form â—Šcode{Z := X}, where â—Šcode{Z != X}, add â—Šcode{{<X,x>,<Z,x>}} to $\lambda$}
      â—Šli{for a statement of the form â—Šcode{Z := X}, where â—Šcode{Z != Y}, add â—Šcode{{<Z,x>,<Y,y>}} to $\lambda$}
    }
  }
}

 Note that the boundary condition here in which â—Šcode{X} is the only variable that gets a constant assigned to it directly must be handled separately. Again, we leave more formal details to the paper (although the paper also glosses over quite a bit of formal detail).

â—Špost-subsection{Filling in the details}
Now, armed with an idea of the general structure of the algorithm, we are ready to fill in some of the details. More specifically, how exactly do we implement each stage as an algorithmic solution to the sub-problems we've described? As a reminder, here are the sub-problems for each stage that give us an idea of â—Šem{how to do} what we've said they â—Šem{should do}:

â—Šol{
  â—Šli{â—Šunderline{Direct assignments:} the â—Šstrong{graph reachability} problem on the realizability graph at that point.}
  â—Šli{â—Šunderline{Copying statements:} the â—Šstrong{disjoint paths with forbidden pairs} problem on the realizability graph at that point.}
  â—Šli{â—Šunderline{Edge computation:} the â—Šstrong{1-CCP (Concurrent Copy Propagation)} problem, using information produced by the â—Šunderline{direct assignments} and â—Šunderline{copying statements} phases.}
  â—Šli{â—Šunderline{Forbidden pair computation:} the â—Šstrong{2-CCP} problem, using information produced by the  â—Šunderline{direct assignments} and â—Šunderline{copying statements} phases.}
}

For â—Šunderline{direct assignments}, remember that we care about statements of the form: â—Šcode{*(d)p = &z}. We will use an algorithmic solution to â—Šstrong{graph reachability} in order to determine if some variable â—Šcode{q} in our current layer $l$ is reachable from â—Šcode{p} (in â—Šcode{d} steps). If so, then â—Šcode{q} can be made to point to â—Šcode{z} (and we can add the edge from â—Šcode{q} to â—Šcode{z}, although the algorithm described in the paper technically delays this until the â—Šunderline{edge computation} phase).

â—Šaside{Note that, by our inductive assumption, we have already computed all edges from layers $L$ to $l+1$ at this point and paths from any layer $>l$ â—Šem{will only use edges from layers $>l$} (this is argued in more detail in the paper, but can sort of intuitively be seen as a corollary of the â—Šem{layered} and â—Šem{acyclic} properties of the realizability). Thus, we have all the information we need to solve graph reachability.}

For â—Šunderline{copying statements}, remember that we only care about statements of the form: â—Šcode{*(d1)p1 = *(d2)p2}. We will use the algorithmic solution to â—Šstrong{disjoint paths with forbidden pairs} to find all pairs â—Šcode{(q1,q2)} in layer $l$ such that there is a disjoint path, avoiding forbidden pairs, from â—Šcode{p1} to â—Šcode{q1} and â—Šcode{p2} to â—Šcode{q2} in our realizability graph so far.

â—Šaside{Again, by our inductive assumption, we have already computed all edges from layers $L$ to $l+1$, as well as all forbidden pairs of edges in each of those layers, so we have all the information we need to solve disjoint paths with forbidden pairs.}

For â—Šunderline{edge computation}, note that the previous two stages have given us: (1) pairs â—Šcode{<q,z>} such that â—Šcode{q} is in layer $l$, â—Šcode{z} is in layer $l-1$, and â—Šcode{q} can be made to point to â—Šcode{z}; and (2) pairs â—Šcode{(q1,q2)} such that values from â—Šcode{q2} can be copied to â—Šcode{q1}. We can then compute edges by solving the 1-CCP problem, where:
â—Šitems{
  â—Šitem{the set of variables $V$ is the set of pointers in layer $l$ of our realizability graph, such that each pointer â—Šcode{q} in layer $l$ becomes a variable â—Šcode{Xq} in the CCP problem}
  â—Šitem{the set of constants $C$ is the set of pointers in layer $l-1$, such that each pointer â—Šcode{z} in layer $l-1$ becomes a constant â—Šcode{az} in the CCP problem}
  â—Šitem{each pair â—Šcode{<q,z>} identified by the â—Šunderline{direct assignments} stage adds statement â—Šcode{Xq := az} to the set of statements $S$ in the CCP problem}
  â—Šitem{each pair â—Šcode{(q1,q2)} identified by the â—Šunderline{copying statements} stage adds statement â—Šcode{X(q1) := X(q2)} to the set of statements $S$ in the CCP problem}
}
Each pair â—Šcode{<Xq,az>} in the solution to the 1-CCP problem as defined here corresponds to an edge â—Šcode{<q,z>} in the graph.

For â—Šunderline{forbidden pair computation}, the correspondence of pointers in the realizability graph and pairs identified by the â—Šunderline{direct assignments} and â—Šunderline{copying statements} stages to $V$, $C$, and $S$ in the CCP problem is the same. Each element â—Šcode{{<X(q1), c(z1)>, <X(q2), c(z2)>}} in the solution to the 2-CCP problem defined this way means that the pair of edges â—Šcode{<q1,z1>,<q2,z2>} â—Šem{can be realized simultaneously}â€”every other pair of realizable edges in layer $l$ (as computed in â—Šunderline{edge computation}) is thus a â—Šstrong{forbidden pair}.

â—Šaside{At this point, all the pieces should be in place for an actual implementation of the algorithm, given a set of well-formed input statements!}

â—Špost-subsection{Missing formalism}
In our narrative here, we've avoided talking at all about any proofs of computational complexity in an effort to focus on a conceptual understanding of â—Šem{how} the algorithm works (rather than â—Šem{why} the algorithm is fast).

The interested reader should refer to â—Šlink["https://dl.acm.org/doi/abs/10.1145/640128.604142"]{the paper itself} for the proofs of the complexity of key components of the algorithm. That said, the paper itself also sweeps some truly rigorous formalism under the rug, particularly when tying all the pieces together at the end of Section 2. The enterprising student would perhaps be interested in working out the full inductive proofs! (I am not an enterprising student.)

â—Šaside{For what it's worth, the hand-wavy argument for polynomial time complexity is basically as follows: (1) the number of layers is linear to the size of the input (i.e. the set of well-formed program statements we get); (2) at each layer, there are 4 stages, each of which can be completed in polynomial time; (3) thus each layer can be completed in polynomial time, (4) meaning the entire algorithm is polynomial time. Finally, (5) all points-to relations (i.e. the point of the algorithm) can simply be read off the graph resulting from the algorithm in constant time, since they are just the edges.}
