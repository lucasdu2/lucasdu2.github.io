<!doctype html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<title>Debugging is dumb</title>
<link rel="stylesheet" type="text/css" href="../normalize.css" />
<link rel="stylesheet" type="text/css" media="all" href="../styles.css" />
<link rel="stylesheet" type="text/css" media="all" href="../pygments-styles.css" />
<link rel="stylesheet" type="text/css" media="all" href="post-styles.css" />
</head>
<body>
  <header><nav><a href="../index.html">Home</a><a href="../posts.html">Posts</a></nav></header>
  <h1>Debugging is dumb</h1><p class="post-date"><post-date class="post-date">Wednesday August 21, 2024</post-date></p><p>Let’s put off doing more meaningful work and write down some quick thoughts I have about debugging, since they happen to be on my mind. All of this is just what I feel right now (and is subject to change). But it’s also something I feel strongly about and that plays a substantial part in what I want to do with my life.</p><h2>Debugging isn’t actually dumb</h2><p>When you talk about debugging, anyone who’s written any kind of computer program probably understands what you mean. It’s likely the most relatable and most inevitable part of software development. Everyone debugs code. Most of software development is, in some sense, debugging.</p><p>Alright. You got me. Debugging isn’t actually dumb! Not really. But it does depend on what you’re talking about when you talk about debugging. I would argue that, while many people hold an idealized view of debugging that is actually quite useful and integral to the software development process, most real-world debugging is not that. The bug finding and bug fixing process in most software, particularly production software, <em>is</em> pretty dumb. The problem, isn’t with the idea of debugging. The problem is that we need to make real-world debugging better.</p><p>The ideal debugging process that people often talk about when discussing debugging involves a kind of bug where there is some essential misunderstanding about the program requirements, about some related system, or about the program itself that needs to be resolved. In these cases, there is something productive happening. You learn something, you get a bit better, perhaps, and the problem is, if you’ve properly understood things, fixed for good.</p><p>This kind of debugging is usually helpful—it’s part of the mistake-feedback-learning loop that can make learning programming so fun and interactive. And this kind of debugging <em>is</em> inevitable and integral to the software development process. No one, no matter how excellent, will produce error-free code within an unfamiliar system. There is always a learning curve.</p><aside>There is a blog post by Allison Kaptur titled <a href="https://akaptur.com/blog/2017/11/12/love-your-bugs/" target="_blank" rel="noopener noreferrer">“Love Your Bugs”</a> that I read a while back and that kicked off some thoughts on this general topic. I found the title of the post a little too saccharine (this probably says something about me). After reading it, I actually did agree with some of what was said, particularly about how bugs can help you learn. That is true, in the ideal (and, I would argue, uncommon) case. I still have issues with the general sentiment of the post—that you <em>should</em> love bugs, otherwise you aren’t going to grow or be a good engineer. Bugs are often incredibly silly, wasteful, and lead to no tangible personal or intellectual growth (except maybe an increasing sense of resignation, which you could spin as “patience”). We don’t have to love them. We should try to look for something better.</aside><p>But most debugging in the real world, in my experience, is not like this. Many misunderstandings that lead to bugs are <em>not</em> about things that are essential, fundamental, or inevitable. Instead, they are results of historical accident. Debugging in real life either:</p><ul><li>does not actually fix the actual problem and results in more bugs and increasing cruft down the line or</li><li>is a result of a misaligned abstraction further down the stack, due to some unprincipled or ad hoc design decision in the system (that might be a bug fix for another, earlier, bug or just an arbitrary, best-guess choice by the system developers).</li></ul><p>Both these things suck. This kind of debugging is dumb and uninteresting. I want my solutions to actually solve the entire problem. I want my solutions to last. And I want the source of the bug to be something foundational, something fundamental that I misunderstood. I don’t want to deal with some arbitrary hack—something that is entirely accidental (à la Fred Brooks, who I love to quote).</p><p>But how do we make this all better?</p><aside>I saw a comment on HackerNews recently about the terrible mess that the Oracle Database codebase is in—a codebase that brings to mind a lot of the frustrations I have with debugging mentioned above—and a response that was essentially like, “OMG, what a great piece of engineering! I can’t believe they can still add features and remain stable and underpin 90% of Fortune 500 companies.” Uhh, no, that should not impress you. That should scare you. That should scare you a lot. You don’t have to accept your terrible reality. You certainly don’t have to celebrate it.</aside><h2>A brief note on assumptions</h2><div class="callout">Bugs are <em>not</em> a natural, fundamental reality of software.</div><p>There are a number of assumptions in software engineering that I think a lot of practitioners, especially more experienced ones, hold. Some of these assumptions are, to me, a little frustrating. They constrict what people think is possible. They limit what people demand of their tools and their software. It’s important, at least to me, to not conflate current reality with natural law.</p><p>One assumption I wish we would abolish is that bugs are some kind of inevitable, natural law of software artifacts. I think the industry is far too accomodating of bugs—or mistakes, really—especially when it is simultaneously trying to push software into more important parts of our lives. Software is, essentially, pure logic. There are no physical realities mandating the presence of bugs. Software can, in theory, be a perfect, perpetual motion machine. That’s part of its appeal.</p><h2>What we (still) lack</h2><p>But we also need to be practical. Humans will, inevitably, make mistakes. What we need are tools, abstractions, and techniques to help us make far fewer mistakes about things that matter—ideally, none at all. In a sense, <em>we need ways to make debugging more pleasant and far more effective.</em></p><p>There are multiple directions from which we can approach this. We should try and create as few bugs as possible (during development—i.e. by-construction), we should try and surface bugs as quickly as possible (especially if they make it into production), and we should make it much easier to fix bugs when they surface.</p><p>I don’t think I’m saying anything radical here, but I do believe that we need far more radical changes to the way we program in order to really accomplish these things in a way that is actually meaningful. We need to push forward both at a developer-facing level (with new products, startups, whatever) and at a deeper, more foundational research level. We need to stop pretending that we already know how best to program and that all we need are some superficial (and possibly LLM-based) tools to help us move faster in paradigms that we’re already comfortable with. We don’t need easier ways to make the same mistakes.</p><p>In particular, I believe we need the following:</p><ul><li>better fitting abstractions—ideally, mathematically principled abstractions—at a systems level, i.e. in the programming languages, databases, file systems, operating systems, etc. that we use (in general, we should aim to solve problems at their essence instead of piling on ad-hoc fixes over the top)</li><li>stronger and broader <em>provable</em> guarantees about important program properties—program proofs should be far more common, starting with the foundational systems we use</li><li>more principled, formal, and automated approaches to testing—i.e. fuzzing, concolic testing, property-based testing, etc.—that are interactive parts of software construction itself (not just some post-facto thing you do when things go wrong in production)</li></ul><p>There are some things being done along these lines in industry that I do respect a lot. What <a href="https://www.antithesis.com/" target="_blank" rel="noopener noreferrer">Antithesis</a> is doing with deterministic testing (which is an extension of the founders’ previous work on FoundationDB’s testing system) is extremely cool and pushes what’s possible in automated testing and debuggability at scale. I like <a href="https://www.instantdb.com/" target="_blank" rel="noopener noreferrer">InstantDBs</a> re-imagining of the database abstraction for client-side collaboration (and also their usage of Clojure!). But we can, and should, go much further. We should try to avoid <a href="https://sourcegraph.com/blog/zig-programming-language-revisiting-design-approach" target="_blank" rel="noopener noreferrer">settling for local maxima</a>.</p><p>I’m personally very interested in mathematical approaches to these things. I really like the promise of formal methods and type systems, both intellectually and practically—broadly, I think proofs and formal methods are the closest thing to a silver bullet that we have. I find a lot of satisfaction in correctness-by-construction. I like the idea of clean-slate abstractions and fundamental rethinking of our tools.</p><p>I just generally like proofs, rigor, and elegance (and yes, I do think those things go together). Maybe that makes me a bad engineer. Maybe that makes me naive and impractical. I don’t know. Honestly, I don’t really care. I guess it does makes sense that I’m so drawn to research, particularly in programming languages and verification. It’s probably a good thing that that’s the direction I’m currently moving in.</p><h2>Fin.</h2><p>One way to sum up is this: debugging is a crucial part of writing software, but it needs to be much more effective. Developing simpler, more fitting abstractions at a systemic level, being able to more easily get formal proofs about our software, and surfacing more errors sooner in the development cycle (preferably at the time of construction) are important pieces of making debugging a more productive and useful endeavor.</p><p>And we need something radically different than the status quo, or at least the status quo that I see. We need far fewer lines of code and far stronger guarantees, particularly about correctness and security, from our software. We’re not going to get that from hacking at things with currently conventional tools.</p><p>Some more food for thought on software quality and verification can be found in Hoare’s <a href="https://6826.csail.mit.edu/2020/papers/noproof.pdf" target="_blank" rel="noopener noreferrer">“How Did Software Get So Reliable Without Proof?”</a> and in some of Dijkstra’s writing and lectures, particularly <a href="https://www.cs.utexas.edu/~EWD/transcriptions/EWD03xx/EWD340.html" target="_blank" rel="noopener noreferrer">“The Humble Programmer.”</a></p><aside>The Dijkstra lecture mentioned above—“The Humble Programmer”—uses “debugging” in a different sense than I do here, which lends weight to my point that there’s no clear and common definition of debugging (making it difficult to discuss properly). I interpret Dijkstra’s use of debugging to indicate post-facto bug fixing, i.e. fixing bugs in existing systems which have already been deployed in production. I’ve instead used debugging in the general sense of fixing errors in a program. In this sense, even programs that are ostensibly correct-by-construction involve debugging during construction, i.e. figuring out why a machine-checked proof is failing.</aside><p>Alright, I’ve already spent far, far too long on this wall of text. Time to get back to work.</p><footer>♦♦♦</footer></body>
</html>